# 9: On-policy Prediction with Approximation

* fn approximator:
  * to approx $v_{\pi}$ from experience generated using a known policy $\pi$
  * $\hat{v}_{\pi}$is represented as a parameterized functional form with weight vector $w \in R^d$;
    **not** as a table anymore
  * $\hat{v}(s,w) \approx v_{\pi}(s)$
  * $\hat{v}$ can be:
    * a linear function in features of the state,
      with $w$ the vector of feature weights.
    * a multi-layer artificial neural network,
      with $w$ the vector of connection weights in all the layers.
    * a decision tree,
      where $w$ is all the numbers defining the split points and leaf values of the tree.
  * the number of weights (the dimensionality of w) is **much less** than the number of states,
    $(d << |S|)$
* generalization
  * changing one weight changes the estimated value of many states
  * when a single state is updated,
    the change generalizes from that state to affect the values of many other states.
* function approximation makes RL applicable to **partially observable** problems,
  * If the parameterized function form for $\hat{v}$ does **not allow**
    the estimated value to depend on certain aspects of the state, then
    it is just **as if** those aspects are unobservable.
* function approximation can **not augment**
  the state representation with **memories of past observations**

## 9.1 Value-function Approximation
* updates $s \mapsto u$
  * $s$: the (arbitrary) state updated,
      * cf $S_t$: state encountered in actual experience
  * $u$: the update target that $s$'s estimated value is shifted toward.
  * natural to interpret each update as
    specifying an example of the desired input-output behavior of the value function;
    supervised learning methods
    * $s \mapsto g$ of each update as a **training example**
  * types of mapping $s \mapsto u$:
    * DP policy-evaluation update: $s \mapsto E_{\pi}[ R_{t+1} + \gamma \hat{v}(S_{t+1},w_t) | S_t=s ]$
    * TD(0) update: $S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1},w_t)$
    * n-step TD update: $S_t \mapsto G_{t:t+n}$
    * Monte Carlo update: $S_t \mapsto G_t$
* On applying supervised learning in RL, need a method that
  * learning be able to occur online,
    while the agent interacts with its environment or with a model of its environment.
    * requires methods that are able to learn efficiently from incrementally acquired data
  * able to handle nonstationary target functions (target functions that change over time).
    * For example, in control methods based on GPI (generalized policy iteration)
      * seek to learn $q_{\pi}$ while $\pi$ changes.
    * Even if the policy remains the same, the target values of training examples are
      nonstationary if they are generated by **bootstrapping methods** (DP and TD learning)

## 9.2 The Prediction Objective
* a natural objective function, the Mean Squared Value Error, $\bar{VE}$
  * $\overline{VE} = \sum_{s \in S} \mu(s) [v_{\pi(s)} - \hat{v}(s,w)]^2$ ...(9.1)
    * $\mu(s) \ge 0$ and $\sum_s \mu(s) = 1$,
      * specify state distribution
      * Often it is chosen to be the fraction of time spent in $s$.
      * Under on-policy training this is called the on-policy distribution
  * note: The best value function for better policy is
    **not necessarily** the best for minimizing $\bar{VE}$
    * not yet clear what a more useful alternative goal for value prediction might be
    * not completely clear that the MSE is the right performance objective for RL
    * recall: ultimate purpose—the reason we are learning a value function is
      to find a better policy.
<!-- * The on-policy distribution is
  the fraction of time spent in each state normalized to sum to one
    * $\mu(s) = \frac{\nu(s)}{\sum_{s'} \nu(s')}$
      * $\nu(s)$: the number of time steps spent, on average, in state $s$ in a single episode -->

## 9.3 Stochastic-gradient and Semi-gradient Methods
* Stochastic gradient-descent (SGD) methods
  * to try to minimize error on the observed examples.
  * by adjusting the weight vector after each example **by a small amount** in
    the **direction** that would most reduce the error on that example
  * assuming:
    * states appear in examples with the same distribution, $\mu$, over which
      we are trying to minimize the $\bar{VE}$ as given by (9.1).
    * on each step, we observe a new example $S_t \mapsto v_{\pi}(S_t)$ consisting of
      a (possibly randomly selected) state St and its true value under the policy;
      those states are assumed: **not successive** states from an interaction with the environment
  * $w_{t+1} = w_{t+1} - \frac{1}{2} \alpha \nabla[ v_{\pi}(S_t) - \hat{v}_{\pi}(S_t, w_t) ]^2$ ...(9.4)
  * $w_{t+1} = w_{t+1} + \alpha [ v_{\pi}(S_t) - \hat{v}_{\pi}(S_t, w_t)] \nabla[ \hat{v}_{\pi}(S_t, w_t) ]$ ...(9.5)
  * are "gradient descent" methods because
    the overall step in $w_t$ is proportional to the negative gradient of the example’s squared error (9.4)
  * are called "stochastic" when the update is done on only a single example,
    which might have been selected stochastically.
* SGD when the target output, $U_t \in R$, is **not the true value**, $v_{\pi}(S_t)$
  * $w_{t+1} = w_{t+1} + \alpha [ U_t - \hat{v}_{\pi}(S_t, w_t)] \nabla[ \hat{v}_{\pi}(S_t, w_t) ]$ ...(9.7)
  * If $U_t$ is an unbiased estimate, then $w_t$ is **guaranteed to converge** to
    a local optimum under the usual stochastic approximation conditions (2.7) for decreasing $\alpha$.
  * eg: Monte Carlo target $U_t = G_t$ is by definition an unbiased estimate of $v_{\pi}(S_t)$
    * ...because the true value ofa state is the expected value of the return following it
    * The gradient-descent version of Monte Carlo state-value prediction is guaranteed to
      find a locally optimal solution
  * One does not obtain the same guarantees if a bootstrapping estimate of $v_{\pi}(S_t)$ is
    used as the target $U_t$ in (9.7).
    * Bootstrapping targets such as
      * n-step returns
      * DP target
    * because they depend on the current value of the weight vector
      * implies that they will be biased and will not value produce of the true gradient-descent method.
* semi-gradient (bootstrapping) methods.
  * use a bootstrapping estimate of $v_{\pi}(S_t)$ as the target $U_t$ in (9.7).
    * $U_t$ will be biased and will not produce of a true gradient-descent method.
  * Bootstrapping methods are not in fact instances of true gradient descent (Barnard, 1993).
    * take into account the effect of changing the weight vector on the estimate,
      but ignore its effect on the target
  * pros and cons
    * (+) enable significantly faster learning,
      * recall: the TD methods are often of vastly reduced variance compared to Monte Carlo methods
    * (+) enable learning to be continual and online, without waiting for the end of an episode.
    * (-) do not converge as robustly as gradient methods
      * do converge reliably in important cases such as the linear case
  * eg: semi-gradient TD(0),
    * target: $U_t = R_{t+1} + \gamma \hat{v}_{\pi}(S_{t+1}, w)$
* State aggregation is a special case of SGD (9.7)

## 9.4 Linear Methods
* the approximate function is a linear function of the weight vector
* For linear methods, features are basis functions because
  they form a linear basis for the set of approximate functions.
  * Constructing d-dimensional feature vectors to represent states is
    the same as selecting a set of d basis functions.
* SGD updates with linear function approximation.
  * $\nabla \hat{v}(s,w) = x(s)$
* the gradient Monte Carlo algorithm presented in the previous
  section converges to the global optimum of the VE under linear function approximation
  if $\alpha$ is reduced over time according to the usual conditions.
* Convergence of Linear TD(0)
  * The semi-gradient TD(0) algorithm presented in the previous section also converges
    under linear function approximation,
  * linear semi-gradient TD(0) converges to the TD fixed point
  * linear semi-gradient DP also converges to the TD fixed point
  * Critical to the these convergence results is
    * that states are updated according to the *on-policy* distribution.
    * For other update distributions, bootstrapping methods using
      function approximation may actually diverge to infinity.

## 9.5 Feature Construction for Linear Methods
* Linear methods
  * (+) convergence guarantees,
  * (+) can be very efficient in terms of both data and computation
  * (-) cannot take into account any interactions between features,
    * such as the presence of feature i being good only in the absence of feature j.
* Feature Construction
  * Polynomials
  * Fourier Basis
  * Coarse Coding
  * Tile Coding
  * Radial Basis Functions

## 9.6 Selecting Step-Size Parameters Manually
there is a similar rule that gives similar behavior in the case
of linear function approximation. Suppose you wanted to learn in about ⌧ experiences
with substantially the same feature vector. A good rule of thumb for setting the step-size
parameter of linear SGD methods is then Equ 9.19

## 9.7 Nonlinear Function Approximation: Artificial Neural Networks
* If an ANN has **at least one loop** in its connections, it is a **recurrent** rather than a feedforward ANN.
* both feedforward and recurrent ANNs have been used in reinforcement learning
* backpropagation algorithm, which consists of alternating forward and backward passes through the network.
  * Each forward pass computes the activation of each unit given the current activations of the network’s input units.
  * After each forward pass, a backward pass efficiently computes a partial derivative for each weight.
* training a network with k + 1 hidden layers can actually result in poorer performance than training
  a network with k hidden layers, even though the deeper network can represent all the
  functions that the shallower network can (Bengio, 2009). Explaining results like these
  is not easy, but several factors are important.
  * First, the large number of weights in
    a typical deep ANN makes it difficult to avoid the problem of overfitting, that is, the
    problem of failing to generalize correctly to cases on which the network has not been trained.
  * Second, backpropagation does not work well for deep ANNs because the partial
    derivatives computed by its backward passes either decay rapidly toward the input side
    of the network, making learning by deep layers extremely slow, or the partial derivatives
    grow rapidly toward the input side of the network, making learning unstable.
* reducing overfitting
  * stopping training when performance begins to decrease on
    validation data different from the training data (cross validation),
  * modifying the objective function to discourage complexity of the approximation (regularization), and
  * introducing dependencies among the weights to reduce the number of degrees of freedom (e.g., weight sharing).
  * dropout method
    * During training, units are randomly removed from the network (dropped out) along with their connections
* Batch normalization (Ioffe and Szegedy, 2015)
  * used statistics from subsets, or “mini-batches,” of training examples to normalize these between-layer
    signals to improve the learning rate of deep ANNs.
* deep residual learning  (He, Zhang, Ren, and Sun, 2016)
  * Sometimes it is easier to learn how a function differs from the identity function than to learn the function itself.
  * adding this difference, or residual function, to the input produces the desired function
* deep convolutional network
* notice p226
> It (Overfitting) is less of a problem for online reinforcement learning that does not rely on limited training sets, but
generalizing e↵ectively is still an important issue.

## 9.8 Least-Squares TD
TODO

## 9.9 Memory-based Function Approximation
* simply save training examples in memory as they arrive (or at least save a subset of the examples)
  without updating any parameters.
  * Then, whenever a query state’s value estimate is needed,
    a set of examples is retrieved from memory and used to compute a value estimate for the query state.
  * are prime examples of nonparametric methods.
* eg: nearest neighbor method, locally weighted regression

## 9.10 Kernel-based Function Approximation
TODO

## 9.11 Looking Deeper at On-policy Learning: Interest and Emphasis
* In some cases, however, we are more interested in some states than others.
  * In discounted episodic problems, for example,
    * more interested in accurately valuing early states in the episode than
      in later states where discounting may have made the rewards much less important to
      the value of the start state. Or,
    * if an action-value function is being learned, it may be less important to
      accurately value poor actions whose value is much less than the greedy action.
* Rather than having one on-policy distribution for the MDP, we will have many.
  * All of them will have in common that they are a distribution of states encountered in
    trajectories while following the target policy, but they will vary in how the trajectories are, in a sense, initiated.
* a non-negative scalar measure, a random variable $I_t$ called **interest**,
  * indicating the degree to which we are interested in accurately valuing
    the state (or state–action pair) at time t.
  * eg: it may depend on the trajectory up to time t or the learned parameters at time t.
* a non-negative scalar random variable, the **emphasis** $M_t$
  * multiplies the learning update and
  * thus emphasizes or de-emphasizes the learning done at time $t$.

## 9.12 Summary
* Reinforcement learning systems must be capable of generalization
  * use supervised-learning function approximation
    by treating each update as a training example.
