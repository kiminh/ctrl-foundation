# Following Newton Direction in Policy Gradient with Parameter Exploration
* https://ieeexplore.ieee.org/document/7280673

# problem
* the PGPE inherits the main drawback of
gradient approaches, i.e., that the choice of the step size
can affect both performance and convergence.

# observ
* the RL literature has posed little attention to higher-order methods.

# background
* The connection between the Natural
gradient and second-{)rder methods resides in the fact that
when the cost function is quadratic, the Fisher information
matrix is equal to the Hessian matrix, and thus Newton's and
the natural gradient methods are identical [15].

# comment
* not with neural nets
* hard to read, seems not well-written in English
