# Asynchronous Methods for Deep Reinforcement Learning
* Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,
  David Silver, Koray Kavukcuoglu
* icml2016: oral
* http://proceedings.mlr.press/v48/mniha16.html
* https://arxiv.org/abs/1602.01783
* https://icml.cc/2016/reviews/885.txt
* https://www.youtube.com/watch?v=Ajjc08-iPx8&feature=youtu.be (for robots, goto: 0:44)
* https://www.youtube.com/watch?v=9sx1_u2qVhQ&t=27s

## problem
* **drawbacks** of Deep RL algorithms based on **experience replay**
  * it uses **more** memory and computation per real interaction; and
  * it **requires off-policy** learning algorithms that can update from data generated by an **older policy**

## observation
* multiple actors-learners **running in parallel** are likely to be **exploring different parts** of the environment.
  * use **different exploration policies** in each actor-learner to maximize this diversity

## idea: A3C framework
* uses
  * **asynchronous gradient descent** for  optimization of deep neural network controllers.
  * asynchronously execute multiple agents in parallel on multiple instances of the environment
    (instead of experience replay)
    * parallelism **decorrelates** the agents’ data into a more stationary process, since
      at any given time-step the parallel agents will be experiencing a variety of different states.
    * multiple **CPU threads** on a single machine (instead of using separate machines and a parameter server)
      * **removes** the communication costs of sending gradients and parameters over multiple machine, cf Gorila framework
      * enables us to use **Hogwild!** (Recht et al., 2011) style updates for training
* running different exploration policies in different threads,
  * the overall changes being made to the parameters are likely to be less correlated in time
    than a single agent applying online updates, hence
    * do not use a replay memory and
    * rely on parallel actors employing different exploration policies to perform the stabilizing role
      undertaken by experience replay in the DQN training algorithm.
  * obtain a reduction in training time that is roughly linear in the number of parallel actor-learners
  * able to use on-policy reinforcement learning methods such as Sarsa and actor-critic to
    train neural networks in a stable way (since we no longer rely on experience replay for stabilizing learning)
* 4 multi-threaded asynchronous variants
  * Asynchronous one-step Q-learning
    * Each thread interacts with its own copy of the environment and
      at each step computes a gradient of the Q-learning loss.
    * accumulate gradients over multiple timesteps before they are applied
      * reduces the chances of multiple actor learners overwriting each other’s updates.
      * provides some ability to trade off computational efficiency for data efficiency
    * giving each thread a different exploration policy helps improve robustness
  * Asynchronous one-step Sarsa
    * same as asynchronous one-step Q-learning except that it uses a different target value for Q(s, a).
  * Asynchronous n-step Q-learning
    * operates in the forward view by explicitly computing n-step returns
  * Asynchronous advantage actor-critic (A3C)
    * maintains a policy `\pi(a_t|s_t; \theta)`  and an estimate of the value function `V(s_t; \theta_v)`
    * operates in the **forward view** and uses the same mix of **n-step returns** to
      update both the policy and the value-function.
    * while the parameters `\theta` of the policy and `\theta_v` of the value function are
      shown as being separate for generality, always **share some of the parameters** in practice
    * typically use a convolutional neural network that has (with all non-output layers shared)
      * one softmax output for the policy `\pi(a_t|s_t; \theta)` and
      * one linear output for the value function `V(s_t; \theta_v)`
    * adding the entropy of the policy to the objective function to improve exploration by
      discouraging premature convergence to suboptimal deterministic policies

## setup
* task:
  * Atari domain (via ALE),
    * int the discrete action domain, the action output is a Softmax
  * TORCS 3D car racing simulator
  * Continuous Action Control Using the MuJoCo:
    * a set of rigid body physics domains with contact dynamics:
      pole swing-up, quadruped locomotion, planar biped walking, balancing, 2D/3D target reaching
    * input for nets:
      * the physical state: joint positions and velocities, the target position
      * RGB pixel inputs
    * two outputs of the policy network are two real number vectors which we treat as
      the mean vector `$\mu$` and scalar variance `$\sigma^2$`of a multidimensional normal distribution with
      a spherical covariance.
      * To act, the input is passed through the model to the output layer where
        we sample from the normal distribution determined by `$\mu$` and `$\sigma^2$`
     * In our experiments with continuous control problems,
       the networks for policy network and value network do **not share** any parameters
       Finally, since the episodes were typically at most several hundred time steps long,
     * did not use any bootstrapping in the policy or value function updates and
       batched each episode into a single update.
  * exploring 3D mazes purely from visual inputs via Deepmind's Labyrinth
* 3 different optimization algorithms in our asynchronous framework
  * SGD with momentum,
  * RMSProp (Tieleman & Hinton, 2012) without shared statistics, and
  * RMSProp with shared statistics.

## result
* The best performing method, **an asynchronous variant of actor-critic**, surpasses
  the current state-of-the-art on the Atari domain while training for
  half the time on **a single multi-core CPU** instead of a GPU.
* using parallel actorlearners to update a shared model had
  a stabilizing effect on the learning process of the three value-based methods
* asynchronous one-step Q-learning and Sarsa algorithms exhibit superlinear speedups that
  cannot be explained by purely computational gains.
  * one-step methods (one-step Q and one-step Sarsa) often require less data to
    achieve a particular score when using more parallel actor-learners;
    due to positive effect of multiple threads to reduce the bias in one-step methods.
* our proposed framework
  * scales well with the number of parallel workers, making efficient use of resources.
  * stable and do not collapse or diverge once they are learning
  * make stable training of neural networks through reinforcement learning possible with
    both value- based and policy-based methods, off-policy as well as on- policy methods, and
    in discrete as well as continuous domains

## background
* unstability of deepNN on RL caused by
  * the sequence of observed data encountered by an online RL agent is non-stationary, and
  * online RL updates are strongly correlated.
* approaches to stabilize deep neural networks on online RL algorithms:
  * by storing the agent’s data in an experience replay memory,
    the data can be batched (Riedmiller, 2005; Schulman et al., 2015a) or
    randomly sampled (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) from different time-steps.
  * Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but
    at the same time limits the methods to off-policy reinforcement learning algorithms.
* One-step Q-learning
  * updates the action value `$Q(s, a)$` toward the one-step return `$r + \lambda max_{a'} Q(s', a'; \theta)$`.
  * One drawback:
    * that obtaining a reward r only directly affects the value of the state action pair s, a that led to the reward.
    * The values of other state action pairs are affected only indirectly through the updated value Q(s, a).
    * make the learning process slow since many updates are required to propagate a reward to
      the relevant preceding states and actions.
  * One way of propagating rewards faster is by using **n-step returns**
    * a single reward r directly affecting the values of n preceding state action pairs.
  * Advantage fn: `$A_{\pi}(s,a) = Q_{\pi}(s,a) - V_{\pi}(s)$`

## comment
 * promising way to use multi-threading in planning where we do simulate multiple env+agent interactions,
   note, from Figure S4: the training time is in the order of hours, meaning **no way** for online planning,
   but what if for offline planning?
 * plots and tables (on the paper, not the appendix) with analysis are all from Atari game setup,
   although there are experiments in/with Mujoco/robots (shown in the appendix)
 * in the [demo vid on robots, 0:44](https://www.youtube.com/watch?v=Ajjc08-iPx8&feature=youtu.be),
    initial state is **fixed**, what if **any** initial state?
 * in a3c, for the critic: what if using Q or (Q-V), instead of V?
